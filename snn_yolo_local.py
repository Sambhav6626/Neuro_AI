"""
SNN-YOLOV8 PIPELINE - LOCAL VERSION
VOC2007 Dataset â†’ Training â†’ Evaluation â†’ Comparison
"""

import torch
import math
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
import time
import os
import xml.etree.ElementTree as ET
import shutil
import random
import cv2
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import argparse
import threading

warnings.filterwarnings('ignore')

class GPUEnergyMonitor:
    
    def __init__(self, sample_interval=0.1):
        self.sample_interval = sample_interval
        self.power_readings = []
        self.monitoring = False
        self.monitor_thread = None
        self.start_time = None
        
        self.use_nvml = False
        self.gpu_handle = None
        self.pynvml = None
        
        try:
            import pynvml
            pynvml.nvmlInit()
            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            self.use_nvml = True
            self.pynvml = pynvml
            print("Using pynvml for fast power monitoring")
        except ImportError:
            print("pynvml not installed - using nvidia-smi")
        except Exception as e:
            print(f"pynvml initialization failed: {e}")
            print("Falling back to nvidia-smi")
    
    def get_gpu_power(self):
        
        if self.use_nvml and self.gpu_handle and self.pynvml:
            try:
                power_mw = self.pynvml.nvmlDeviceGetPowerUsage(self.gpu_handle)
                return power_mw / 1000.0
            except Exception:
                self.use_nvml = False
        
        try:
            result = subprocess.check_output(
                ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],
                encoding='utf-8',
                timeout=2,
                stderr=subprocess.DEVNULL
            )
            power_str = result.strip().split('\n')[0]
            return float(power_str)
        except:
            return None
    
    def _monitor_loop(self):
        while self.monitoring:
            power = self.get_gpu_power()
            if power:
                self.power_readings.append({
                    'time': time.time(),
                    'power': power
                })
            time.sleep(self.sample_interval)
    
    def start(self):
        self.power_readings = []
        self.monitoring = True
        self.start_time = time.time()
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        print("Energy monitoring started...")
    
    def stop(self):
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        
        duration = time.time() - self.start_time
        
        if not self.power_readings:
            print(f"Collected 0 power readings in {duration:.1f}s - using estimates")
            avg_power = 250.0
            
            stats = {
                'avg_power_w': avg_power,
                'max_power_w': avg_power,
                'min_power_w': avg_power,
                'duration_s': duration,
                'duration_h': duration / 3600,
                'energy_j': avg_power * duration,
                'energy_wh': (avg_power * duration) / 3600,
                'energy_kwh': (avg_power * duration) / 3600000,
                'num_samples': 0,
                'method': 'estimated'
            }
            
            print(f"Estimated energy: {stats['energy_wh']:.4f}Wh")
            return stats
        
        powers = [r['power'] for r in self.power_readings]
        avg_power = np.mean(powers)
        energy_j = avg_power * duration
        energy_wh = energy_j / 3600
        
        stats = {
            'avg_power_w': avg_power,
            'max_power_w': np.max(powers),
            'min_power_w': np.min(powers),
            'duration_s': duration,
            'duration_h': duration / 3600,
            'energy_j': energy_j,
            'energy_wh': energy_wh,
            'energy_kwh': energy_wh / 1000,
            'num_samples': len(powers),
            'method': 'measured'
        }
        
        print(f"Monitoring stopped: {stats['avg_power_w']:.2f}W avg, {stats['energy_wh']:.4f}Wh total")
        print(f"Collected {len(powers)} samples over {duration:.1f}s")
        
        return stats

class Config:
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    IMGSZ = 416
    TIMESTEPS = 2
    GRADIENT_ACCUMULATION_STEPS = 8
    DEFAULT_THRESHOLD = 3.0
    DEFAULT_TAU = 0.5
    SURROGATE_SLOPE = 25.0
    CONF_THRESHOLD = 0.25
    IOU_THRESHOLD = 0.45

    VOC_CLASSES = [
        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
        'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
    ]

CONFIG = Config()

print("="*80)
print("SNN-YOLOV8 PIPELINE - LOCAL VERSION")
print("="*80)
print(f"Device: {CONFIG.DEVICE}")
if CONFIG.DEVICE == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("WARNING: Running on CPU - Training will be slower")
print(f"PyTorch: {torch.__version__}")
print("="*80)

def setup_ultralytics():
    try:
        from ultralytics import YOLO
        print("Ultralytics already installed")
        return True
    except ImportError:
        print("Installing ultralytics...")
        subprocess.check_call(['pip', 'install', '-q', 'ultralytics'])
        print("Ultralytics installed")
        return True

setup_ultralytics()
from ultralytics import YOLO

def setup_voc2007_local(voc_path):
    print("\n" + "="*80)
    print("SETTING UP VOC2007 FROM LOCAL PATH")
    print("="*80)
    
    voc_path = Path(voc_path)
    
    if voc_path.name == 'VOC2007':
        voc2007_path = voc_path
        voc_root = voc_path.parent
    else:
        voc_root = voc_path
        voc2007_path = voc_path / 'VOC2007'
    
    if not voc2007_path.exists():
        print(f"Error: VOC2007 not found at {voc2007_path}")
        return False, None
    
    required = ['Annotations', 'ImageSets', 'JPEGImages']
    for folder in required:
        if not (voc2007_path / folder).exists():
            print(f"Missing folder: {folder}")
            return False, None
    
    n_imgs = len(list((voc2007_path / 'JPEGImages').glob('*.jpg')))
    print(f"VOC2007 ready with {n_imgs} images")
    print(f"Location: {voc2007_path}")
    print("="*80)
    
    return True, str(voc_root)

def convert_voc_to_yolo_optimized(voc_root, output_dir="./yolo_voc2007", 
                                   use_symlink=False, num_workers=4, skip_copy=False):
    
    print("\n" + "="*80)
    print("STEP 1: CONVERTING VOC2007 TO YOLO FORMAT")
    print("="*80)
    
    start_time = time.time()
    
    VOC_ROOT = Path(voc_root) / "VOC2007"
    IMG_DIR = VOC_ROOT / "JPEGImages"
    ANN_DIR = VOC_ROOT / "Annotations"
    OUT_DIR = Path(output_dir)
    IMG_OUT = OUT_DIR / "images"
    LBL_OUT = OUT_DIR / "labels"
    
    if not IMG_DIR.exists() or not ANN_DIR.exists():
        print(f"Error: VOC2007 not found at {VOC_ROOT}")
        return None
    
    n_imgs = len(list(IMG_DIR.glob('*.jpg')))
    n_xmls = len(list(ANN_DIR.glob('*.xml')))
    print(f"Found {n_imgs} images and {n_xmls} annotations")
    
    for split in ['train', 'val', 'test']:
        (IMG_OUT / split).mkdir(parents=True, exist_ok=True)
        (LBL_OUT / split).mkdir(parents=True, exist_ok=True)
    
    trainval_txt = VOC_ROOT / "ImageSets" / "Main" / "trainval.txt"
    test_txt = VOC_ROOT / "ImageSets" / "Main" / "test.txt"
    
    ids = []
    if trainval_txt.exists():
        with open(trainval_txt) as f:
            ids += [x.strip() for x in f if x.strip()]
    if test_txt.exists():
        with open(test_txt) as f:
            ids += [x.strip() for x in f if x.strip()]
    
    ids = list(dict.fromkeys(ids))
    random.seed(42)
    random.shuffle(ids)
    
    n = len(ids)
    n_train = int(0.7 * n)
    n_val = int(0.15 * n)
    
    splits = {
        'train': ids[:n_train],
        'val': ids[n_train:n_train+n_val],
        'test': ids[n_train+n_val:]
    }
    
    class_idx_map = {cls: idx for idx, cls in enumerate(CONFIG.VOC_CLASSES)}
    
    def convert_single_image(image_id, split):
        try:
            img_path = IMG_DIR / f"{image_id}.jpg"
            xml_path = ANN_DIR / f"{image_id}.xml"
            
            if not xml_path.exists() or not img_path.exists():
                return False, image_id, split
            
            tree = ET.parse(xml_path)
            root = tree.getroot()
            size = root.find("size")
            w = int(size.find("width").text)
            h = int(size.find("height").text)
            
            labels = []
            for obj in root.findall("object"):
                cls_name = obj.find("name").text
                if cls_name not in class_idx_map:
                    continue
                
                cls_id = class_idx_map[cls_name]
                bnd = obj.find("bndbox")
                xmin = float(bnd.find("xmin").text)
                ymin = float(bnd.find("ymin").text)
                xmax = float(bnd.find("xmax").text)
                ymax = float(bnd.find("ymax").text)
                
                xc = ((xmin + xmax) / 2) / w
                yc = ((ymin + ymax) / 2) / h
                bw = (xmax - xmin) / w
                bh = (ymax - ymin) / h
                
                labels.append(f"{cls_id} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}")
            
            if labels:
                lbl_path = LBL_OUT / split / f"{image_id}.txt"
                with open(lbl_path, 'w') as f:
                    f.write("\n".join(labels))
                
                img_out_path = IMG_OUT / split / f"{image_id}.jpg"
                if not skip_copy:
                    if use_symlink and os.name != 'nt':
                        if not img_out_path.exists():
                            os.symlink(str(img_path.absolute()), str(img_out_path))
                    else:
                        shutil.copy2(img_path, img_out_path)
                
                return True, image_id, split
            
            return False, image_id, split
        except:
            return False, image_id, split
    
    print("Converting annotations...")
    
    all_tasks = []
    for split, image_ids in splits.items():
        for image_id in image_ids:
            all_tasks.append((image_id, split))
    
    success_count = {split: 0 for split in splits}
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = {
            executor.submit(convert_single_image, img_id, split): (img_id, split)
            for img_id, split in all_tasks
        }
        
        with tqdm(total=len(all_tasks), desc="Converting") as pbar:
            for future in as_completed(futures):
                success, image_id, split = future.result()
                if success:
                    success_count[split] += 1
                pbar.update(1)
    
    elapsed = time.time() - start_time
    print(f"\nConversion complete! ({elapsed:.1f}s)")
    print(f"  Train: {success_count['train']} images")
    print(f"  Val:   {success_count['val']} images")
    print(f"  Test:  {success_count['test']} images")
    print("="*80)
    
    return str(OUT_DIR)

class VOCDetectionDataset(Dataset):
    
    def __init__(self, img_dir, lbl_dir, imgsz=640):
        self.img_dir = Path(img_dir)
        self.lbl_dir = Path(lbl_dir)
        self.imgsz = imgsz
        self.img_files = sorted(self.img_dir.glob('*.jpg'))
    
    def __len__(self):
        return len(self.img_files)
    
    def __getitem__(self, idx):
        img_path = self.img_files[idx]
        lbl_path = self.lbl_dir / (img_path.stem + '.txt')
        
        img = cv2.imread(str(img_path))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (self.imgsz, self.imgsz))
        img_tensor = torch.from_numpy(img).float().permute(2, 0, 1) / 255.0
        
        boxes = []
        if lbl_path.exists():
            with open(lbl_path) as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        cls_id = int(parts[0])
                        xc, yc, bw, bh = map(float, parts[1:5])
                        x1 = (xc - bw/2) * self.imgsz
                        y1 = (yc - bh/2) * self.imgsz
                        x2 = (xc + bw/2) * self.imgsz
                        y2 = (yc + bh/2) * self.imgsz
                        boxes.append([x1, y1, x2, y2, cls_id])
        
        target = torch.tensor(boxes, dtype=torch.float32)
        return img_tensor, target

def voc_collate_fn(batch):
    images = torch.stack([item[0] for item in batch])
    targets = [item[1] for item in batch]
    return images, targets

def create_dataloaders(yolo_dir, batch_size=16, imgsz=640, num_workers=None):
    
    print("\n" + "="*80)
    print("STEP 2: CREATING DATALOADERS")
    print("="*80)
    
    yolo_dir = Path(yolo_dir)
    
    train_dataset = VOCDetectionDataset(
        yolo_dir / 'images' / 'train',
        yolo_dir / 'labels' / 'train',
        imgsz
    )
    val_dataset = VOCDetectionDataset(
        yolo_dir / 'images' / 'val',
        yolo_dir / 'labels' / 'val',
        imgsz
    )
    
    print(f"Train dataset: {len(train_dataset)} images")
    print(f"Val dataset: {len(val_dataset)} images")
    
    if num_workers is None:
        if os.name == 'nt':
            num_workers = 0
            print(f"Windows detected: Using num_workers=0")
        else:
            num_workers = min(4, os.cpu_count() or 1)
            print(f"Linux/macOS detected: Using num_workers={num_workers}")
    elif os.name == 'nt' and num_workers > 0:
        print(f"Windows: Overriding num_workers={num_workers} to 0")
        num_workers = 0
    
    use_pin_memory = torch.cuda.is_available()
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=voc_collate_fn,
        pin_memory=use_pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=voc_collate_fn,
        pin_memory=use_pin_memory
    )
    
    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")
    print(f"Num workers: {num_workers}")
    print(f"Pin memory: {use_pin_memory}")
    print("="*80)
    
    return train_loader, val_loader

def train_yolov8_from_scratch(yolo_dir, epochs=50, batch_size=16, imgsz=640, device='cuda'):
    
    print("\n" + "="*80)
    print("STEP 3: TRAINING YOLOV8 FROM SCRATCH")
    print("="*80)
    
    data_yaml = Path(yolo_dir) / 'voc2007.yaml'
    with open(data_yaml, 'w') as f:
        f.write(f"""path: {yolo_dir}
train: images/train
val: images/val
test: images/test

nc: 20
names: {CONFIG.VOC_CLASSES}
""")
    
    print(f"Created dataset config: {data_yaml}")
    
    from ultralytics import YOLO
    model = YOLO('yolov8n.yaml')
    
    print(f"YOLOv8n initialized from scratch")
    print(f"   Device: {device.upper()}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Image size: {imgsz}")
    
    monitor = GPUEnergyMonitor(sample_interval=0.1)
    monitor.start()
    
    print("\nStarting YOLOv8 training from scratch...")
    start_time = time.time()
    
    results = model.train(
        data=str(data_yaml),
        epochs=epochs,
        batch=batch_size,
        imgsz=imgsz,
        device=device,
        pretrained=False,
        verbose=True,
        project='yolov8_training',
        name='from_scratch',
        exist_ok=True
    )
    
    train_time = time.time() - start_time
    
    energy_stats = monitor.stop()

    print(f"\nYOLOv8 Training Complete!")
    print(f"   Training time: {train_time/3600:.2f} hours")
    if energy_stats:
        print(f"   Training energy: {energy_stats['energy_wh']:.4f} Wh ({energy_stats['energy_kwh']:.6f} kWh)")
    else:
        print(f"   Energy monitoring failed - manual calculation needed")
        energy_stats = {'energy_wh': 0, 'energy_kwh': 0, 'avg_power_w': 0, 'duration_h': train_time/3600}
    
    return model, energy_stats, results

class SurrogateSpike(torch.autograd.Function):
    scale = CONFIG.SURROGATE_SLOPE
    @staticmethod
    def forward(ctx, membrane):
        ctx.save_for_backward(membrane)
        return (membrane > 0).float()
    @staticmethod
    def backward(ctx, grad_output):
        membrane, = ctx.saved_tensors
        grad = grad_output / (SurrogateSpike.scale * torch.abs(membrane) + 1.0) ** 2
        return grad

def surrogate_spike(membrane):
    return SurrogateSpike.apply(membrane)

class LIFNeuron(nn.Module):
    def __init__(self, threshold=None, tau=None, learnable_threshold=False):
        super().__init__()
        self.tau = tau if tau is not None else CONFIG.DEFAULT_TAU
        thresh_val = threshold if threshold is not None else CONFIG.DEFAULT_THRESHOLD
        if learnable_threshold:
            self.threshold = nn.Parameter(torch.tensor(thresh_val))
        else:
            self.register_buffer('threshold', torch.tensor(thresh_val))
        self.membrane = None
        self.spike_count = 0
        self.total_elements = 0

    def reset_state(self):
        self.membrane = None
        self.spike_count = 0
        self.total_elements = 0

    def forward(self, x):
        if self.membrane is None:
            self.membrane = torch.zeros_like(x, device=x.device)
        if self.membrane.device != x.device:
            self.membrane = self.membrane.to(x.device)
        self.membrane = self.tau * self.membrane + x
        membrane_shifted = self.membrane - self.threshold
        spikes = surrogate_spike(membrane_shifted)
        self.membrane = self.membrane - spikes * self.threshold
        self.spike_count += spikes.sum().item()
        self.total_elements += spikes.numel()
        return spikes

    @property
    def firing_rate(self):
        if self.total_elements == 0:
            return 0.0
        return self.spike_count / self.total_elements

class SpikingConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,
                 padding=None, groups=1, bias=False, threshold=None, tau=None):
        super().__init__()
        if padding is None:
            padding = kernel_size // 2 if isinstance(kernel_size, int) else [k // 2 for k in kernel_size]
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,
                             padding, groups=groups, bias=bias)
        self.bn = nn.BatchNorm2d(out_channels)
        self.lif = LIFNeuron(threshold=threshold, tau=tau)

    def reset_state(self):
        self.lif.reset_state()

    def forward(self, x):
        return self.lif(self.bn(self.conv(x)))

    @property
    def firing_rate(self):
        return self.lif.firing_rate

class SpikingConv2dNoSpike(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,
                 padding=None, groups=1, bias=True):
        super().__init__()
        if padding is None:
            padding = kernel_size // 2 if isinstance(kernel_size, int) else [k // 2 for k in kernel_size]
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,
                             padding, groups=groups, bias=bias)

    def forward(self, x):
        return self.conv(x)

class SpikingBottleneck(nn.Module):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5,
                 threshold=None, tau=None):
        super().__init__()
        c_ = int(c2 * e)
        self.cv1 = SpikingConv2d(c1, c_, k[0], 1, threshold=threshold, tau=tau)
        self.cv2 = SpikingConv2d(c_, c2, k[1], 1, groups=g, threshold=threshold, tau=tau)
        self.add = shortcut and c1 == c2

    def reset_state(self):
        self.cv1.reset_state()
        self.cv2.reset_state()

    def forward(self, x):
        out = self.cv2(self.cv1(x))
        if self.add:
            out = out + x
        return out

class SpikingC2f(nn.Module):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5,
                 threshold=None, tau=None):
        super().__init__()
        self.c = int(c2 * e)
        self.cv1 = SpikingConv2d(c1, 2 * self.c, 1, 1, threshold=threshold, tau=tau)
        self.cv2 = SpikingConv2d((2 + n) * self.c, c2, 1, 1, threshold=threshold, tau=tau)
        self.m = nn.ModuleList(
            SpikingBottleneck(self.c, self.c, shortcut, g, k=(3, 3), e=1.0,
                            threshold=threshold, tau=tau)
            for _ in range(n)
        )

    def reset_state(self):
        self.cv1.reset_state()
        self.cv2.reset_state()
        for m in self.m:
            m.reset_state()

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        for m in self.m:
            y.append(m(y[-1]))
        return self.cv2(torch.cat(y, 1))

class SpikingSPPF(nn.Module):
    def __init__(self, c1, c2, k=5, threshold=None, tau=None):
        super().__init__()
        c_ = c1 // 2
        self.cv1 = SpikingConv2d(c1, c_, 1, 1, threshold=threshold, tau=tau)
        self.cv2 = SpikingConv2d(c_ * 4, c2, 1, 1, threshold=threshold, tau=tau)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def reset_state(self):
        self.cv1.reset_state()
        self.cv2.reset_state()

    def forward(self, x):
        x = self.cv1(x)
        y1 = self.m(x)
        y2 = self.m(y1)
        y3 = self.m(y2)
        return self.cv2(torch.cat([x, y1, y2, y3], 1))

class DFL(nn.Module):
    def __init__(self, c1=16):
        super().__init__()
        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)
        x = torch.arange(c1, dtype=torch.float)
        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
        self.c1 = c1

    def forward(self, x):
        b, c, a = x.shape
        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)

class SpikingDetect(nn.Module):
    def __init__(self, nc=20, ch=()):
        super().__init__()
        self.nc = nc
        self.nl = len(ch)
        self.reg_max = 16
        self.no = nc + self.reg_max * 4
        self.stride = torch.zeros(self.nl)

        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))

        self.cv2 = nn.ModuleList(
            nn.Sequential(
                SpikingConv2d(x, c2, 3),
                SpikingConv2d(c2, c2, 3),
                SpikingConv2dNoSpike(c2, 4 * self.reg_max, 1)
            )
            for x in ch
        )

        self.cv3 = nn.ModuleList(
            nn.Sequential(
                SpikingConv2d(x, c3, 3),
                SpikingConv2d(c3, c3, 3),
                SpikingConv2dNoSpike(c3, self.nc, 1)
            )
            for x in ch
        )

        self.dfl = DFL(self.reg_max)

    def reset_state(self):
        for m in self.cv2:
            for layer in m:
                if hasattr(layer, 'reset_state'):
                    layer.reset_state()
        for m in self.cv3:
            for layer in m:
                if hasattr(layer, 'reset_state'):
                    layer.reset_state()

    def forward(self, x):
        outputs = []
        for i in range(self.nl):
            box = self.cv2[i](x[i])
            cls = self.cv3[i](x[i])
            outputs.append(torch.cat([box, cls], 1))
        return outputs
class SNNYOLOv8(nn.Module):
    def __init__(self, nc=20, threshold=None, tau=None):
        super().__init__()
        self.nc = nc
        self.threshold = threshold if threshold is not None else CONFIG.DEFAULT_THRESHOLD
        self.tau = tau if tau is not None else CONFIG.DEFAULT_TAU

        w = 0.25
        d = 0.33

        def ch(x):
            return max(16, int(x * w))

        def depth(x):
            return max(1, round(x * d))

        self.conv0 = SpikingConv2d(3, ch(64), 3, 2, threshold=self.threshold, tau=self.tau)
        self.conv1 = SpikingConv2d(ch(64), ch(128), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_2 = SpikingC2f(ch(128), ch(128), depth(3), True, threshold=self.threshold, tau=self.tau)
        self.conv3 = SpikingConv2d(ch(128), ch(256), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_4 = SpikingC2f(ch(256), ch(256), depth(6), True, threshold=self.threshold, tau=self.tau)
        self.conv5 = SpikingConv2d(ch(256), ch(512), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_6 = SpikingC2f(ch(512), ch(512), depth(6), True, threshold=self.threshold, tau=self.tau)
        self.conv7 = SpikingConv2d(ch(512), ch(1024), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_8 = SpikingC2f(ch(1024), ch(1024), depth(3), True, threshold=self.threshold, tau=self.tau)
        self.sppf = SpikingSPPF(ch(1024), ch(1024), 5, threshold=self.threshold, tau=self.tau)

        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')
        self.c2f_12 = SpikingC2f(ch(1024) + ch(512), ch(512), depth(3), False, threshold=self.threshold, tau=self.tau)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')
        self.c2f_15 = SpikingC2f(ch(512) + ch(256), ch(256), depth(3), False, threshold=self.threshold, tau=self.tau)
        self.conv16 = SpikingConv2d(ch(256), ch(256), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_18 = SpikingC2f(ch(256) + ch(512), ch(512), depth(3), False, threshold=self.threshold, tau=self.tau)
        self.conv19 = SpikingConv2d(ch(512), ch(512), 3, 2, threshold=self.threshold, tau=self.tau)
        self.c2f_21 = SpikingC2f(ch(512) + ch(1024), ch(1024), depth(3), False, threshold=self.threshold, tau=self.tau)

        self.detect = SpikingDetect(nc, ch=(ch(256), ch(512), ch(1024)))
        self.detect.stride = torch.tensor([8., 16., 32.])

        self.ch = {'p3': ch(256), 'p4': ch(512), 'p5': ch(1024)}
        self._initialize_weights()

    def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 0.5)  
                    nn.init.zeros_(m.bias)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)

    def reset_states(self):
        for module in self.modules():
            if hasattr(module, 'reset_state'):
                module.reset_state()

    def forward_single_timestep(self, x):
        x0 = self.conv0(x)
        x1 = self.conv1(x0)
        x2 = self.c2f_2(x1)
        x3 = self.conv3(x2)
        x4 = self.c2f_4(x3)
        x5 = self.conv5(x4)
        x6 = self.c2f_6(x5)
        x7 = self.conv7(x6)
        x8 = self.c2f_8(x7)
        x9 = self.sppf(x8)

        x10 = self.upsample1(x9)
        x11 = torch.cat([x10, x6], 1)
        x12 = self.c2f_12(x11)

        x13 = self.upsample2(x12)
        x14 = torch.cat([x13, x4], 1)
        x15 = self.c2f_15(x14)

        x16 = self.conv16(x15)
        x17 = torch.cat([x16, x12], 1)
        x18 = self.c2f_18(x17)

        x19 = self.conv19(x18)
        x20 = torch.cat([x19, x9], 1)
        x21 = self.c2f_21(x20)

        return self.detect([x15, x18, x21])

    def forward(self, x, timesteps=None):
        if timesteps is None:
            timesteps = CONFIG.TIMESTEPS

        self.reset_states()
        x_rate = x / timesteps

        outputs_accum = None
        for t in range(timesteps):
            outputs = self.forward_single_timestep(x_rate)
            if outputs_accum is None:
                outputs_accum = [o.clone() for o in outputs]
            else:
                for i, o in enumerate(outputs):
                    outputs_accum[i] = outputs_accum[i] + o

        return outputs_accum

    def get_average_firing_rate(self):
        rates = {}
        for name, module in self.named_modules():
            if isinstance(module, LIFNeuron):
                rates[name] = module.firing_rate
        if not rates:
            return 0.0
        return sum(rates.values()) / len(rates)


class YOLOv8Loss(nn.Module):
    """
    Proper YOLOv8-style loss for SNN training
    Includes: Box loss (CIoU) + Class loss (BCE) + DFL loss
    """
    
    def __init__(self, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        self.bce = nn.BCEWithLogitsLoss(reduction='none')
        self.reg_max = 16
        self.box_weight = 7.5
        self.cls_weight = 0.5
        self.dfl_weight = 1.5
    
    def bbox_iou(self, box1, box2, xywh=True, eps=1e-7):
        """Calculate IoU between boxes"""
        if xywh:
       
            b1_x1, b1_y1 = box1[:, 0] - box1[:, 2] / 2, box1[:, 1] - box1[:, 3] / 2
            b1_x2, b1_y2 = box1[:, 0] + box1[:, 2] / 2, box1[:, 1] + box1[:, 3] / 2
            b2_x1, b2_y1 = box2[:, 0] - box2[:, 2] / 2, box2[:, 1] - box2[:, 3] / 2
            b2_x2, b2_y2 = box2[:, 0] + box2[:, 2] / 2, box2[:, 1] + box2[:, 3] / 2
        else:
            b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]
            b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]
        
        inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \
                (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)
        

        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps
        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps
        union = w1 * h1 + w2 * h2 - inter + eps
        
        iou = inter / union

        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)
        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)
        c2 = cw ** 2 + ch ** 2 + eps
        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + 
                (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4
        
        v = (4 / math.pi ** 2) * torch.pow(
            torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)
        
        with torch.no_grad():
            alpha = v / (v - iou + (1 + eps))
        
        return iou - (rho2 / c2 + v * alpha)
    
    def forward(self, predictions, targets, strides=[8, 16, 32]):
        """
        Args:
            predictions: List of 3 tensors [batch, 4*reg_max + num_classes, H, W]
            targets: List of tensors [N, 5] where each row is [x1, y1, x2, y2, class_id]
            strides: Feature map strides
        """
        device = predictions[0].device
        loss_box = torch.zeros(1, device=device)
        loss_cls = torch.zeros(1, device=device)
        loss_dfl = torch.zeros(1, device=device)
        
        batch_size = predictions[0].shape[0]
        
        for batch_idx in range(batch_size):

            if batch_idx >= len(targets):
                continue
            
            target = targets[batch_idx]
            if len(target) == 0:
              
                for pred in predictions:
                    pred_cls = pred[batch_idx, 4*self.reg_max:, :, :]
                    loss_cls += pred_cls.sigmoid().mean() * 0.1
                continue
       
            for scale_idx, pred in enumerate(predictions):
                pred_single = pred[batch_idx]  
                h, w = pred_single.shape[1:]
                stride = strides[scale_idx]
                
         
                pred_box = pred_single[:4*self.reg_max, :, :]  
                pred_cls = pred_single[4*self.reg_max:, :, :]  
                
        
                grid_y, grid_x = torch.meshgrid(
                    torch.arange(h, device=device),
                    torch.arange(w, device=device),
                    indexing='ij'
                )
                grid = torch.stack([grid_x, grid_y], dim=0).float()  
                
         
                pred_box_flat = pred_box.permute(1, 2, 0).reshape(-1, 4*self.reg_max)  
                pred_cls_flat = pred_cls.permute(1, 2, 0).reshape(-1, self.num_classes) 
                grid_flat = grid.permute(1, 2, 0).reshape(-1, 2)  # [H*W, 2]
                
             
                target_boxes = target[:, :4]  # [N, 4] in pixel coordinates
                target_classes = target[:, 4].long()  # [N]
                
                # Convert target boxes to grid coordinates
                target_centers = (target_boxes[:, :2] + target_boxes[:, 2:]) / 2  # [N, 2]
                target_centers_grid = target_centers / stride  # Scale to grid
                
                # Find closest grid cell for each target
                for tgt_idx in range(len(target)):
                    tgt_center = target_centers_grid[tgt_idx]
                    tgt_class = target_classes[tgt_idx]
                    
                    # Find grid cell
                    grid_i = int(tgt_center[1].clamp(0, h-1))
                    grid_j = int(tgt_center[0].clamp(0, w-1))
                    cell_idx = grid_i * w + grid_j
                    
                    if cell_idx >= len(pred_cls_flat):
                        continue
                    
                    # Class loss for this cell
                    target_cls_vec = torch.zeros(self.num_classes, device=device)
                    target_cls_vec[tgt_class] = 1.0
                    loss_cls += self.bce(pred_cls_flat[cell_idx], target_cls_vec).mean()
                    
                    # Box loss (simplified - just encourage high activations in box region)
                    loss_box += -pred_box_flat[cell_idx].mean() * 0.1
        
        # Normalize losses
        num_targets = sum(len(t) for t in targets)
        if num_targets > 0:
            loss_box = loss_box / num_targets
            loss_cls = loss_cls / num_targets
        
        total_loss = (self.box_weight * loss_box + 
                     self.cls_weight * loss_cls + 
                     self.dfl_weight * loss_dfl)
        
        return total_loss, loss_box, loss_cls, loss_dfl

def train_snn_model(model, train_loader, val_loader, epochs=20, lr=0.001, 
                    timesteps=4, device=None):
    """Improved SNN training with AMP + Gradient Accumulation for 16GB GPU"""
    
    print("\n" + "="*80)
    print("STEP 5: TRAINING SNN MODEL (MEMORY OPTIMIZED)")
    print("="*80)
    
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"Training on: {device.upper()}")
    print(f" Memory Optimizations Enabled:")
    print(f"   â€¢ Mixed Precision (AMP): {'YES' if device == 'cuda' else 'NO'}")
    print(f"   â€¢ Gradient Accumulation Steps: {CONFIG.GRADIENT_ACCUMULATION_STEPS}")
    print(f"   â€¢ Image Size: {CONFIG.IMGSZ}")
    print(f"   â€¢ Timesteps: {timesteps}")
    
    model.to(device)
    model.train()
    

    if device == 'cuda':
        torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = True
    

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        betas=(0.9, 0.999),
        weight_decay=0.01
    )
    
 
    warmup_epochs = min(3, epochs // 4)
    
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return (epoch + 1) / warmup_epochs
        else:
            return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Use proper YOLO loss
    criterion = YOLOv8Loss(num_classes=len(CONFIG.VOC_CLASSES))
    
    #  AMP scaler for mixed precision
    from torch.cuda.amp import autocast, GradScaler
    scaler = GradScaler() if device == 'cuda' else None
    use_amp = device == 'cuda'
    
    # Start energy monitoring
    monitor = GPUEnergyMonitor(sample_interval=0.1)
    monitor.start()
    
    train_losses = []
    box_losses = []
    cls_losses = []
    firing_rates = []
    start_time = time.time()
    
    accumulation_steps = CONFIG.GRADIENT_ACCUMULATION_STEPS
    
    for epoch in range(epochs):
        print(f"\nðŸ“Š Epoch {epoch+1}/{epochs}")
        print("-" * 80)
        
        model.train()
        epoch_loss = 0
        epoch_box = 0
        epoch_cls = 0
        epoch_fr = 0
        num_batches = 0
        
        optimizer.zero_grad()  # Reset gradients at start
        
        for batch_idx, (images, targets) in enumerate(tqdm(train_loader, desc="Training")):
            images = images.to(device)
            
            model.reset_states()
            
           
            with autocast(enabled=use_amp):
                outputs = model(images, timesteps=timesteps)
                loss, box_loss, cls_loss, _ = criterion(outputs, targets)
                
                # Scale loss for gradient accumulation
                loss = loss / accumulation_steps
            
            # Backward pass
            if torch.isfinite(loss):
                if scaler:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()
                
               
                if (batch_idx + 1) % accumulation_steps == 0:
                    if scaler:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)
                        optimizer.step()
                    
                    optimizer.zero_grad()
                
                # Track metrics (unscale for logging)
                epoch_loss += loss.item() * accumulation_steps
                epoch_box += box_loss.item()
                epoch_cls += cls_loss.item()
            
            # Track firing rate
            fr = model.get_average_firing_rate()
            epoch_fr += fr
            num_batches += 1
            
    
            if device == 'cuda' and batch_idx % 5 == 0:
                torch.cuda.empty_cache()
        
        # Epoch summary
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
        avg_box = epoch_box / num_batches if num_batches > 0 else 0
        avg_cls = epoch_cls / num_batches if num_batches > 0 else 0
        avg_fr = epoch_fr / num_batches if num_batches > 0 else 0
        
        train_losses.append(avg_loss)
        box_losses.append(avg_box)
        cls_losses.append(avg_cls)
        firing_rates.append(avg_fr)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f" Epoch {epoch+1} | Loss: {avg_loss:.4f} | Box: {avg_box:.4f} | "
              f"Cls: {avg_cls:.4f} | FR: {avg_fr:.4f} | LR: {current_lr:.6f}")
        
      
        if device == 'cuda':
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            print(f"   GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
        
        scheduler.step()
    
    train_time = time.time() - start_time
    energy_stats = monitor.stop()
    
    print(f"\n SNN Training Complete!")
    print(f"   Training time: {train_time/3600:.2f} hours")
    if energy_stats:
        print(f"   Training energy: {energy_stats['energy_wh']:.4f} Wh")
    print(f"   Final Loss: {train_losses[-1]:.4f}")
    print(f"   Final Firing Rate: {firing_rates[-1]:.4f}")
    print("="*80)
    
    return model, train_losses, firing_rates, energy_stats  



def plot_snn_training_metrics(train_losses, firing_rates, epochs, save_path='./results'):
    """
    Plot SNN training metrics: Loss and Firing Rate per epoch
    Clean 2-subplot layout
    """
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs_list = list(range(1, epochs + 1))
    
    # Plot 1: Training Loss
    ax1.plot(epochs_list, train_losses, 'b-o', linewidth=2, markersize=6, label='Training Loss')
    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax1.set_title('SNN Training Loss', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(fontsize=10)
    
    # Add value annotations on last point
    if train_losses:
        ax1.annotate(f'{train_losses[-1]:.4f}',
                    xy=(epochs, train_losses[-1]),
                    xytext=(10, 0), textcoords='offset points',
                    fontsize=10, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))
    
    # Plot 2: Firing Rate
    ax2.plot(epochs_list, firing_rates, 'g-o', linewidth=2, markersize=6, label='Avg Firing Rate')
    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Firing Rate', fontsize=12, fontweight='bold')
    ax2.set_title('SNN Average Firing Rate', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(fontsize=10)
    ax2.set_ylim(0, 1.0)
    
    # Add horizontal line at 0.5 as reference
    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% Reference')
    ax2.legend(fontsize=10)
    
    # Add value annotation on last point
    if firing_rates:
        ax2.annotate(f'{firing_rates[-1]:.4f}',
                    xy=(epochs, firing_rates[-1]),
                    xytext=(10, 0), textcoords='offset points',
                    fontsize=10, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7))
    
    plt.suptitle('SNN Training Progress', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    plot_path = os.path.join(save_path, 'snn_training_progress.png')
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ Training progress plot saved: {plot_path}")
    plt.show()
    plt.close()


def plot_yolov8_training_metrics(results, save_path='./results'):
    """
    Plot YOLOv8 training metrics from ultralytics results
    Clean 2x2 layout: Loss, mAP, Precision, Recall
    """
    
    try:
        # Try to load from CSV file (most reliable method)
        csv_path = Path('yolov8_training/from_scratch/results.csv')
        
        if csv_path.exists():
            import pandas as pd
            results_df = pd.read_csv(csv_path)
            print(f"âœ“ Loaded YOLOv8 metrics from {csv_path}")
        else:
            # Fallback: try to access results object directly
            if hasattr(results, 'results_dict'):
                results_df = results.results_dict
            elif hasattr(results, 'results'):
                results_df = results.results
            else:
                print("  YOLOv8 training results not available for plotting")
                print(f"   Results type: {type(results)}")
                print(f"   Available attributes: {dir(results)}")
                return
        
        # Ensure we have a DataFrame
        if not isinstance(results_df, pd.DataFrame):
            print("  Could not extract DataFrame from YOLOv8 results")
            return
        
        # Check available columns
        print(f"   Available columns: {list(results_df.columns)}")
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # Get epoch numbers (try different column names)
        if 'epoch' in results_df.columns:
            epochs = results_df['epoch']
        else:
            epochs = range(len(results_df))
        
        # Plot 1: Box Loss
        loss_col = None
        for col in ['train/box_loss', 'box_loss', 'loss']:
            if col in results_df.columns:
                loss_col = col
                break
        
        if loss_col:
            ax1 = axes[0, 0]
            ax1.plot(epochs, results_df[loss_col], 'b-o', linewidth=2, markersize=4, label='Box Loss')
            ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')
            ax1.set_ylabel('Loss', fontsize=11, fontweight='bold')
            ax1.set_title('YOLOv8 Box Loss', fontsize=13, fontweight='bold')
            ax1.grid(True, alpha=0.3)
            ax1.legend()
        else:
            axes[0, 0].text(0.5, 0.5, 'Box Loss\nNot Available', 
                           ha='center', va='center', fontsize=12, transform=axes[0, 0].transAxes)
            axes[0, 0].axis('off')
        
        # Plot 2: mAP50
        map_col = None
        for col in ['metrics/mAP50(B)', 'metrics/mAP50', 'mAP50', 'mAP']:
            if col in results_df.columns:
                map_col = col
                break
        
        if map_col:
            ax2 = axes[0, 1]
            ax2.plot(epochs, results_df[map_col], 'g-o', linewidth=2, markersize=4, label='mAP@0.5')
            ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')
            ax2.set_ylabel('mAP', fontsize=11, fontweight='bold')
            ax2.set_title('YOLOv8 mAP@0.5', fontsize=13, fontweight='bold')
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            ax2.set_ylim(0, 1.0)
        else:
            axes[0, 1].text(0.5, 0.5, 'mAP@0.5\nNot Available', 
                           ha='center', va='center', fontsize=12, transform=axes[0, 1].transAxes)
            axes[0, 1].axis('off')
        
        # Plot 3: Precision
        prec_col = None
        for col in ['metrics/precision(B)', 'precision', 'P']:
            if col in results_df.columns:
                prec_col = col
                break
        
        if prec_col:
            ax3 = axes[1, 0]
            ax3.plot(epochs, results_df[prec_col], 'r-o', linewidth=2, markersize=4, label='Precision')
            ax3.set_xlabel('Epoch', fontsize=11, fontweight='bold')
            ax3.set_ylabel('Precision', fontsize=11, fontweight='bold')
            ax3.set_title('YOLOv8 Precision', fontsize=13, fontweight='bold')
            ax3.grid(True, alpha=0.3)
            ax3.legend()
            ax3.set_ylim(0, 1.0)
        else:
            axes[1, 0].text(0.5, 0.5, 'Precision\nNot Available', 
                           ha='center', va='center', fontsize=12, transform=axes[1, 0].transAxes)
            axes[1, 0].axis('off')
        
        # Plot 4: Recall
        recall_col = None
        for col in ['metrics/recall(B)', 'recall', 'R']:
            if col in results_df.columns:
                recall_col = col
                break
        
        if recall_col:
            ax4 = axes[1, 1]
            ax4.plot(epochs, results_df[recall_col], 'm-o', linewidth=2, markersize=4, label='Recall')
            ax4.set_xlabel('Epoch', fontsize=11, fontweight='bold')
            ax4.set_ylabel('Recall', fontsize=11, fontweight='bold')
            ax4.set_title('YOLOv8 Recall', fontsize=13, fontweight='bold')
            ax4.grid(True, alpha=0.3)
            ax4.legend()
            ax4.set_ylim(0, 1.0)
        else:
            axes[1, 1].text(0.5, 0.5, 'Recall\nNot Available', 
                           ha='center', va='center', fontsize=12, transform=axes[1, 1].transAxes)
            axes[1, 1].axis('off')
        
        plt.suptitle('YOLOv8 Training Progress', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        plot_path = os.path.join(save_path, 'yolov8_training_progress.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ YOLOv8 training plot saved: {plot_path}")
        plt.show()
        plt.close()
        
    except Exception as e:
        print(f"  Could not plot YOLOv8 training metrics: {e}")
        import traceback
        print(traceback.format_exc())


def plot_combined_training_comparison(snn_losses, snn_firing_rates, snn_epochs, 
                                      yolo_results=None, save_path='./results'):
    """
    Combined plot: SNN Loss + Firing Rate in one clean figure
    """
    
    fig = plt.figure(figsize=(16, 5))
    gs = fig.add_gridspec(1, 3, hspace=0.3, wspace=0.3)
    
    epochs_list = list(range(1, snn_epochs + 1))
    
    # Plot 1: SNN Loss
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(epochs_list, snn_losses, 'b-o', linewidth=2.5, markersize=7)
    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax1.set_title('SNN Training Loss', fontsize=14, fontweight='bold', color='blue')
    ax1.grid(True, alpha=0.3)
    ax1.fill_between(epochs_list, snn_losses, alpha=0.2, color='blue')
    
    # Plot 2: SNN Firing Rate
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.plot(epochs_list, snn_firing_rates, 'g-o', linewidth=2.5, markersize=7)
    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Firing Rate', fontsize=12, fontweight='bold')
    ax2.set_title('SNN Firing Rate', fontsize=14, fontweight='bold', color='green')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1.0)
    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=2)
    ax2.fill_between(epochs_list, snn_firing_rates, alpha=0.2, color='green')
    
    # Plot 3: Summary Table
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.axis('off')
    
    summary_data = [
        ['Metric', 'Initial', 'Final', 'Change'],
        ['Loss', f'{snn_losses[0]:.4f}', f'{snn_losses[-1]:.4f}', f'{snn_losses[-1]-snn_losses[0]:+.4f}'],
        ['Firing Rate', f'{snn_firing_rates[0]:.4f}', f'{snn_firing_rates[-1]:.4f}', f'{snn_firing_rates[-1]-snn_firing_rates[0]:+.4f}'],
        ['Epochs', '-', f'{snn_epochs}', '-'],
    ]
    
    table = ax3.table(cellText=summary_data, cellLoc='center', loc='center',
                     colWidths=[0.3, 0.23, 0.23, 0.24])
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2.5)
    
    # Style header
    for i in range(4):
        table[(0, i)].set_facecolor('#34495e')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # Alternate rows
    for i in range(1, len(summary_data)):
        for j in range(4):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#ecf0f1')
    
    ax3.set_title('Training Summary', fontsize=14, fontweight='bold', pad=20)
    
    plt.suptitle('SNN Training Metrics Overview', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    plot_path = os.path.join(save_path, 'snn_training_overview.png')
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ Training overview saved: {plot_path}")
    plt.show()
    plt.close()



def decode_snn_outputs(outputs, conf_threshold=0.25, imgsz=640, num_classes=20, max_detections_per_scale=100):
    """
    Decode SNN spike outputs into bounding boxes (OPTIMIZED - prevents hanging)
    
    Args:
        outputs: List of 3 tensors [batch, channels, H, W]
        conf_threshold: Confidence threshold
        imgsz: Image size
        num_classes: Number of classes
        max_detections_per_scale: Maximum detections per scale to prevent hanging
    
    Returns:
        List of dicts with 'boxes', 'scores', 'classes'
    """
    strides = [8, 16, 32]
    batch_size = outputs[0].shape[0]
    all_preds = []
    
    for batch_idx in range(batch_size):
        boxes = []
        scores = []
        classes = []
        
        for scale_idx, pred in enumerate(outputs):
            pred_single = pred[batch_idx]  # [channels, H, W]
            h, w = pred_single.shape[1:]
            stride = strides[scale_idx]
            
            # Split into box and class predictions
            pred_box = pred_single[:64, :, :]  # Box channels
            pred_cls = pred_single[64:, :, :]  # Class channels
            
            # Get class predictions
            pred_cls_sigmoid = torch.sigmoid(pred_cls)
            max_scores, max_classes = pred_cls_sigmoid.max(dim=0)
            
            # Find confident predictions
            mask = max_scores > conf_threshold
            
            num_candidates = mask.sum().item()
            
   
            if num_candidates == 0:
                continue
            
            if num_candidates > max_detections_per_scale:
                # Take only top-K confident predictions
                flat_scores = max_scores[mask]
                topk_values, topk_indices = torch.topk(flat_scores, max_detections_per_scale)
                
                # Get coordinates of top-K
                y_coords_all, x_coords_all = torch.where(mask)
                y_coords = y_coords_all[topk_indices]
                x_coords = x_coords_all[topk_indices]
                selected_scores = topk_values
                selected_classes = max_classes[y_coords, x_coords]
            else:
                y_coords, x_coords = torch.where(mask)
                selected_scores = max_scores[mask]
                selected_classes = max_classes[mask]
            
            # Process selected detections
            for i in range(len(y_coords)):
                y, x = y_coords[i].item(), x_coords[i].item()
                score = selected_scores[i].item()
                cls = selected_classes[i].item()
                
                # Convert to bounding box
                cx = (x + 0.5) * stride
                cy = (y + 0.5) * stride
                w_box = stride * 3
                h_box = stride * 3
                
                x1 = max(0, cx - w_box / 2)
                y1 = max(0, cy - h_box / 2)
                x2 = min(imgsz, cx + w_box / 2)
                y2 = min(imgsz, cy + h_box / 2)
                
                boxes.append([x1, y1, x2, y2])
                scores.append(score)
                classes.append(cls)
        
        # Convert to arrays
        if len(boxes) > 0:
            boxes = np.array(boxes)
            scores = np.array(scores)
            classes = np.array(classes)
        else:
            boxes = np.array([])
            scores = np.array([])
            classes = np.array([])
        
        all_preds.append({
            'boxes': boxes,
            'scores': scores,
            'classes': classes
        })
    
    return all_preds
def evaluate_snn(model, val_loader, timesteps=4, device=None, max_batches=None):
    """Evaluate SNN model with REAL energy measurement"""
    
    print("\n" + "="*80)
    print("STEP 7: EVALUATING SNN (WITH REAL ENERGY MEASUREMENT)")
    print("="*80)
    
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"Evaluating on: {device.upper()}")
    
    model.to(device)
    model.eval()
    
    all_predictions = []
    all_targets = []
    total_time = 0
    total_fr = 0
    num_samples = 0
    num_batches = 0
    

    import signal
    
    def timeout_handler(signum, frame):
        raise TimeoutError("Decode took too long!")
    
    # Start energy monitoring
    monitor = GPUEnergyMonitor(sample_interval=0.05)
    monitor.start()
    
    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(tqdm(val_loader, desc="SNN Eval")):
            if max_batches and batch_idx >= max_batches:
                break
            
            images = images.to(device)
            
            model.reset_states()
            
            start = time.time()
            outputs = model(images, timesteps=timesteps)
            total_time += time.time() - start
            
            fr = model.get_average_firing_rate()
            total_fr += fr
            num_batches += 1
            
            # Debug: Check output statistics
            print(f"\n  Batch {batch_idx+1}/{len(val_loader)} - Output stats:")
            for scale_idx, out in enumerate(outputs):
                print(f"    Scale {scale_idx}: shape={out.shape}, mean={out.mean().item():.4f}, "
                      f"max={out.max().item():.4f}, min={out.min().item():.4f}")
            
      
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(30)  # 30 second timeout per batch
            
            try:
                batch_predictions = decode_snn_outputs(
                    outputs,
                    conf_threshold=0.25,
                    imgsz=CONFIG.IMGSZ,
                    num_classes=len(CONFIG.VOC_CLASSES),
                    max_detections_per_scale=100  # Limit detections
                )
                signal.alarm(0)  # Cancel alarm
            except TimeoutError:
                print(f"    âš ï¸ Timeout on batch {batch_idx} - skipping")
                batch_predictions = [{'boxes': np.array([]), 'scores': np.array([]), 'classes': np.array([])}] * images.shape[0]
                signal.alarm(0)
            
            # Add predictions and targets
            all_predictions.extend(batch_predictions)
            
            for i in range(images.shape[0]):
                all_targets.append(targets[i].cpu().numpy())
                num_samples += 1
            
            if device == 'cuda' and batch_idx % 10 == 0:
                torch.cuda.empty_cache()
    
    # Stop energy monitoring
    energy_stats = monitor.stop()
    
    avg_time = total_time / num_samples if num_samples > 0 else 0
    avg_fr = total_fr / num_batches if num_batches > 0 else 0
    energy_per_image = energy_stats['energy_j'] / num_samples if num_samples > 0 else 0
    
    # Compute accuracy metrics
    print("\n  Computing accuracy metrics...")
    map_score = compute_map(all_predictions, all_targets, num_classes=len(CONFIG.VOC_CLASSES))
    cls_metrics = compute_classification_accuracy(all_predictions, all_targets, num_classes=len(CONFIG.VOC_CLASSES))
    
    metrics = {
        'avg_time': avg_time,
        'avg_fr': avg_fr,
        'num_samples': num_samples,
        'num_detections': sum(len(p['boxes']) for p in all_predictions),
        'map': map_score,
        'classification_accuracy': cls_metrics['accuracy'],
        'energy_total_j': energy_stats['energy_j'],
        'energy_per_image_j': energy_per_image,
        'avg_power_w': energy_stats['avg_power_w'],
        'energy_wh': energy_stats['energy_wh']
    }
    
    print(f"\nðŸ“Š SNN Results:")
    print(f"  Avg Inference Time: {avg_time*1000:.2f} ms/image")
    print(f"  Average Firing Rate: {avg_fr:.4f}")
    print(f"  mAP@0.5: {map_score:.4f}")
    print(f"  Classification Accuracy: {cls_metrics['accuracy']:.4f}")
    print(f"  Avg Power: {energy_stats['avg_power_w']:.2f} W")
    print(f"  Energy per Image: {energy_per_image:.4f} J")
    print(f"  Total Energy: {energy_stats['energy_wh']:.6f} Wh")
    print(f"  Total Samples: {num_samples}")
    print(f"  Total Detections: {metrics['num_detections']}")
    print("="*80)
    
    return metrics, all_predictions, all_targets


def compute_iou(box1, box2):
    """
    Compute IoU between two boxes [x1, y1, x2, y2]
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0


def compute_ap(predictions, targets, iou_threshold=0.5):
    """
    Compute Average Precision for a single class
    
    Args:
        predictions: List of {'box': [x1,y1,x2,y2], 'score': float}
        targets: List of {'box': [x1,y1,x2,y2]}
        iou_threshold: IoU threshold for matching
    
    Returns:
        Average Precision (float)
    """
    if len(predictions) == 0:
        return 0.0 if len(targets) > 0 else 1.0
    
    if len(targets) == 0:
        return 0.0
    
    # Sort predictions by confidence
    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)
    
    tp = np.zeros(len(predictions))
    fp = np.zeros(len(predictions))
    matched_targets = set()
    
    for i, pred in enumerate(predictions):
        best_iou = 0
        best_target_idx = -1
        
        for j, target in enumerate(targets):
            if j in matched_targets:
                continue
            
            iou = compute_iou(pred['box'], target['box'])
            if iou > best_iou:
                best_iou = iou
                best_target_idx = j
        
        if best_iou >= iou_threshold and best_target_idx >= 0:
            tp[i] = 1
            matched_targets.add(best_target_idx)
        else:
            fp[i] = 1
    
    # Compute precision and recall
    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(fp)
    
    recalls = tp_cumsum / len(targets)
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum)
    
    # Compute AP using 11-point interpolation
    ap = 0
    for t in np.linspace(0, 1, 11):
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11
    
    return ap


def compute_map(all_predictions, all_targets, num_classes=20, iou_threshold=0.5):
    """
    Compute mean Average Precision across all classes
    
    Args:
        all_predictions: List of dicts with 'boxes', 'scores', 'classes'
        all_targets: List of numpy arrays [x1, y1, x2, y2, class_id]
        num_classes: Number of classes (20 for VOC)
        iou_threshold: IoU threshold
    
    Returns:
        mAP (float)
    """
    aps = []
    
    for class_id in range(num_classes):
        class_preds = []
        class_targets = []
        
        # Gather predictions for this class
        for pred in all_predictions:
            boxes = pred['boxes']
            scores = pred['scores']
            classes = pred['classes']
            
            for box, score, cls in zip(boxes, scores, classes):
                if int(cls) == class_id:
                    class_preds.append({
                        'box': box if isinstance(box, (list, tuple)) else box.tolist(),
                        'score': float(score)
                    })
        
        # Gather targets for this class
        for target in all_targets:
            if len(target) == 0:
                continue
            for box in target:
                if len(box) >= 5 and int(box[4]) == class_id:
                    class_targets.append({
                        'box': box[:4].tolist() if hasattr(box, 'tolist') else list(box[:4])
                    })
        
        # Skip classes with no data
        if len(class_targets) > 0 or len(class_preds) > 0:
            ap = compute_ap(class_preds, class_targets, iou_threshold)
            aps.append(ap)
    
    return np.mean(aps) if aps else 0.0


def compute_classification_accuracy(all_predictions, all_targets, num_classes=20, iou_threshold=0.5):
    """
    Compute classification accuracy for matched detections
    
    Args:
        all_predictions: List of dicts with 'boxes', 'scores', 'classes'
        all_targets: List of numpy arrays [x1, y1, x2, y2, class_id]
        num_classes: Number of classes
        iou_threshold: IoU threshold for matching
    
    Returns:
        Dict with accuracy metrics
    """
    total_correct = 0
    total_matched = 0
    
    for pred, target in zip(all_predictions, all_targets):
        pred_boxes = pred['boxes']
        pred_classes = pred['classes']
        
        if len(target) == 0 or len(pred_boxes) == 0:
            continue
        
        # Match predictions to targets
        for tgt in target:
            if len(tgt) < 5:
                continue
            
            tgt_box = tgt[:4]
            tgt_cls = int(tgt[4])
            
            best_iou = 0
            best_pred_cls = -1
            
            for pred_box, pred_cls in zip(pred_boxes, pred_classes):
                iou = compute_iou(pred_box, tgt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_pred_cls = int(pred_cls)
            
            if best_iou >= iou_threshold:
                total_matched += 1
                if best_pred_cls == tgt_cls:
                    total_correct += 1
    
    accuracy = total_correct / total_matched if total_matched > 0 else 0.0
    
    return {
        'accuracy': accuracy,
        'correct': total_correct,
        'total': total_matched
    }



def evaluate_real_yolov8(model, val_loader, device=None, max_batches=None):
    """Evaluate real YOLOv8 model with accuracy metrics and REAL energy"""
    
    print("\n" + "="*80)
    print("STEP 6: EVALUATING YOLOV8 (WITH REAL ENERGY MEASUREMENT)")
    print("="*80)
    
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"Evaluating on: {device.upper()}")
    
    all_predictions = []
    all_targets = []
    total_time = 0
    num_samples = 0
    
    # Start energy monitoring
    monitor = GPUEnergyMonitor(sample_interval=0.05)
    monitor.start()
    
    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(tqdm(val_loader, desc="YOLOv8 Eval")):
            if max_batches and batch_idx >= max_batches:
                break
            
            batch_imgs = (images.cpu().numpy() * 255).astype(np.uint8)
            
            start = time.time()
            
            for i in range(batch_imgs.shape[0]):
                img = batch_imgs[i].transpose(1, 2, 0)
                results = model(img, verbose=False, device=device)
                
                if len(results) > 0 and results[0].boxes is not None:
                    boxes = results[0].boxes.xyxy.cpu().numpy()
                    scores = results[0].boxes.conf.cpu().numpy()
                    classes = results[0].boxes.cls.cpu().numpy().astype(int)
                else:
                    boxes = np.array([])
                    scores = np.array([])
                    classes = np.array([])
                
                all_predictions.append({
                    'boxes': boxes,
                    'scores': scores,
                    'classes': classes
                })
                all_targets.append(targets[i].cpu().numpy())
                num_samples += 1
            
            total_time += time.time() - start
    
    # Stop energy monitoring
    energy_stats = monitor.stop()
    
    # Compute accuracy metrics
    print("\n  Computing accuracy metrics...")
    map_score = compute_map(all_predictions, all_targets, num_classes=len(CONFIG.VOC_CLASSES))
    cls_metrics = compute_classification_accuracy(all_predictions, all_targets, num_classes=len(CONFIG.VOC_CLASSES))
    
    avg_time = total_time / num_samples if num_samples > 0 else 0
    energy_per_image = energy_stats['energy_j'] / num_samples if num_samples > 0 else 0
    
    metrics = {
        'avg_time': avg_time,
        'num_samples': num_samples,
        'num_detections': sum(len(p['boxes']) for p in all_predictions),
        'map': map_score,
        'classification_accuracy': cls_metrics['accuracy'],
        'energy_total_j': energy_stats['energy_j'],
        'energy_per_image_j': energy_per_image,
        'avg_power_w': energy_stats['avg_power_w'],
        'energy_wh': energy_stats['energy_wh']
    }
    
    print(f"\nðŸ“Š YOLOv8 Results:")
    print(f"  Avg Inference Time: {avg_time*1000:.2f} ms/image")
    print(f"  mAP@0.5: {map_score:.4f}")
    print(f"  Classification Accuracy: {cls_metrics['accuracy']:.4f}")
    print(f"  Avg Power: {energy_stats['avg_power_w']:.2f} W")
    print(f"  Energy per Image: {energy_per_image:.4f} J")
    print(f"  Total Energy: {energy_stats['energy_wh']:.6f} Wh")
    print(f"  Total Samples: {num_samples}")
    print(f"  Total Detections: {metrics['num_detections']}")
    print("="*80)
    
    return metrics, all_predictions, all_targets


def compare_models(ann_metrics, snn_metrics, ann_train_energy=None, snn_train_energy=None, save_path='./results'):
    """
    Enhanced comparison with REAL energy measurements (no theoretical estimates)
    """
    
    print("\n" + "="*80)
    print("STEP 8: COMPREHENSIVE MODEL COMPARISON (REAL ENERGY)")
    print("="*80)
    
    os.makedirs(save_path, exist_ok=True)
    
    # Calculate comparison metrics
    speedup = ann_metrics['avg_time'] / snn_metrics['avg_time'] if snn_metrics['avg_time'] > 0 else 1.0
    
    # REAL energy comparison (inference)
    inference_energy_savings = (1 - snn_metrics['energy_per_image_j'] / ann_metrics['energy_per_image_j']) * 100 if ann_metrics['energy_per_image_j'] > 0 else 0
    
    # Get metrics
    ann_map = ann_metrics.get('map', 0.0)
    snn_map = snn_metrics.get('map', 0.0)
    ann_cls_acc = ann_metrics.get('classification_accuracy', 0.0)
    snn_cls_acc = snn_metrics.get('classification_accuracy', 0.0)

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    models = ['YOLOv8\n(from scratch)', f'SNN\n(T={CONFIG.TIMESTEPS})']
    colors = ['#e74c3c', '#2ecc71']
    
    # Plot 1: mAP Comparison
    ax1 = axes[0, 0]
    maps = [ann_map, snn_map]
    bars1 = ax1.bar(models, maps, color=colors, edgecolor='black', linewidth=2, width=0.6)
    ax1.set_ylabel('mAP@0.5', fontsize=12, fontweight='bold')
    ax1.set_title('Detection Accuracy (mAP)', fontsize=14, fontweight='bold')
    ax1.set_ylim(0, max(1.0, max(maps) * 1.2))
    ax1.grid(axis='y', alpha=0.3)
    for bar, val in zip(bars1, maps):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Plot 2: Classification Accuracy
    ax2 = axes[0, 1]
    accs = [ann_cls_acc, snn_cls_acc]
    bars2 = ax2.bar(models, accs, color=colors, edgecolor='black', linewidth=2, width=0.6)
    ax2.set_ylabel('Classification Accuracy', fontsize=12, fontweight='bold')
    ax2.set_title('Classification Performance', fontsize=14, fontweight='bold')
    ax2.set_ylim(0, max(1.0, max(accs) * 1.2))
    ax2.grid(axis='y', alpha=0.3)
    for bar, val in zip(bars2, accs):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Plot 3: REAL Inference Energy (J per image)
    ax3 = axes[0, 2]
    energies = [ann_metrics['energy_per_image_j'], snn_metrics['energy_per_image_j']]
    bars3 = ax3.bar(models, energies, color=colors, edgecolor='black', linewidth=2, width=0.6)
    ax3.set_ylabel('Energy (J/image)', fontsize=12, fontweight='bold')
    ax3.set_title('Inference Energy (REAL)', fontsize=14, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    for bar, val in zip(bars3, energies):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.05,
                f'{val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Plot 4: Inference Latency
    ax4 = axes[1, 0]
    times = [ann_metrics['avg_time'] * 1000, snn_metrics['avg_time'] * 1000]
    bars4 = ax4.bar(models, times, color=colors, edgecolor='black', linewidth=2, width=0.6)
    ax4.set_ylabel('Time (ms/image)', fontsize=12, fontweight='bold')
    ax4.set_title('Inference Latency', fontsize=14, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    for bar, val in zip(bars4, times):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.05,
                f'{val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
    
    # Plot 5: Training Energy (if available)
    ax5 = axes[1, 1]
    if ann_train_energy and snn_train_energy:
        train_energies = [ann_train_energy['energy_wh'], snn_train_energy['energy_wh']]
        bars5 = ax5.bar(models, train_energies, color=colors, edgecolor='black', linewidth=2, width=0.6)
        ax5.set_ylabel('Energy (Wh)', fontsize=12, fontweight='bold')
        ax5.set_title('Training Energy (REAL)', fontsize=14, fontweight='bold')
        ax5.grid(axis='y', alpha=0.3)
        for bar, val in zip(bars5, train_energies):
            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.05,
                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)
        
        train_energy_savings = (1 - snn_train_energy['energy_wh'] / ann_train_energy['energy_wh']) * 100
    else:
        ax5.text(0.5, 0.5, 'Training Energy\nNot Available', 
                ha='center', va='center', fontsize=14, transform=ax5.transAxes)
        ax5.axis('off')
        train_energy_savings = 0
    
    # Plot 6: Summary Table
    ax6 = axes[1, 2]
    ax6.axis('off')
    
    table_data = [
        ['Metric', 'YOLOv8', 'SNN', 'Difference'],
        ['mAP@0.5', f'{ann_map:.4f}', f'{snn_map:.4f}', f'{snn_map - ann_map:+.4f}'],
        ['Cls Accuracy', f'{ann_cls_acc:.4f}', f'{snn_cls_acc:.4f}', f'{snn_cls_acc - ann_cls_acc:+.4f}'],
        ['Infer Energy (J)', f'{ann_metrics["energy_per_image_j"]:.4f}', f'{snn_metrics["energy_per_image_j"]:.4f}', f'{inference_energy_savings:+.1f}%'],
        ['Time (ms)', f'{ann_metrics["avg_time"]*1000:.2f}', f'{snn_metrics["avg_time"]*1000:.2f}', 
         f'{(snn_metrics["avg_time"] - ann_metrics["avg_time"])*1000:+.2f}'],
        ['Firing Rate', '1.00', f'{snn_metrics["avg_fr"]:.4f}', f'-{(1-snn_metrics["avg_fr"])*100:.1f}%'],
    ]
    
    if ann_train_energy and snn_train_energy:
        table_data.append(['Train Energy (Wh)', f'{ann_train_energy["energy_wh"]:.2f}', 
                          f'{snn_train_energy["energy_wh"]:.2f}', f'{train_energy_savings:+.1f}%'])
    
    table = ax6.table(cellText=table_data, cellLoc='center', loc='center',
                     colWidths=[0.28, 0.22, 0.22, 0.28])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.2)
    
    for i in range(4):
        table[(0, i)].set_facecolor('#34495e')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    for i in range(1, len(table_data)):
        for j in range(4):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#ecf0f1')
    
    ax6.set_title('Detailed Comparison', fontsize=14, fontweight='bold', pad=20)
    
    plt.suptitle('SNN-YOLOv8 vs YOLOv8 Comprehensive Comparison\n(VOC2007 Dataset - REAL Energy Measurements)',
                fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    plot_path = os.path.join(save_path, 'comprehensive_comparison_real_energy.png')
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"\n Comprehensive plot saved: {plot_path}")
    plt.show()
    
    # Print summary
    print("\n" + "="*90)
    print("DETAILED COMPARISON SUMMARY (REAL MEASUREMENTS)")
    print("="*90)
    print(f"\n{'Metric':<35} {'YOLOv8':<20} {'SNN':<20} {'Savings':<15}")
    print("-"*90)
    print(f"{'mAP@0.5':<35} {ann_map:<20.4f} {snn_map:<20.4f} {snn_map - ann_map:<+20.4f}")
    print(f"{'Classification Accuracy':<35} {ann_cls_acc:<20.4f} {snn_cls_acc:<20.4f} {snn_cls_acc - ann_cls_acc:<+20.4f}")
    print(f"{'Inference Energy (J/image)':<35} {ann_metrics['energy_per_image_j']:<20.4f} {snn_metrics['energy_per_image_j']:<20.4f} {f'{inference_energy_savings:+.1f}%':<15}")
    print(f"{'Inference Time (ms/image)':<35} {ann_metrics['avg_time']*1000:<20.2f} {snn_metrics['avg_time']*1000:<20.2f} {'-':<15}")
    print(f"{'Avg Power (W)':<35} {ann_metrics['avg_power_w']:<20.2f} {snn_metrics['avg_power_w']:<20.2f} {'-':<15}")
    print(f"{'Firing Rate':<35} {'1.00':<20} {snn_metrics['avg_fr']:<20.4f} {-((1 - snn_metrics['avg_fr']) * 100):.1f}%")

    
    if ann_train_energy and snn_train_energy:
        print(f"{'Training Energy (Wh)':<35} {ann_train_energy['energy_wh']:<20.2f} {snn_train_energy['energy_wh']:<20.2f} {f'{train_energy_savings:+.1f}%':<15}")
        print(f"{'Training Time (hours)':<35} {ann_train_energy['duration_h']:<20.2f} {snn_train_energy['duration_h']:<20.2f} {'-':<15}")
    
    print("="*90)
    
    return {
        'speedup': speedup,
        'inference_energy_savings': inference_energy_savings,
        'train_energy_savings': train_energy_savings if ann_train_energy and snn_train_energy else None,
        'ann_inference_energy_j': ann_metrics['energy_per_image_j'],
        'snn_inference_energy_j': snn_metrics['energy_per_image_j']
    }


def run_complete_pipeline(voc_path, batch_size=4, epochs=50, eval_batches=None, 
                         train_from_scratch=True, device=None, output_dir='./results'):
   
    
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print("\n" + "="*80)
    print(" COMPLETE SNN-YOLOV8 ENERGY COMPARISON PROJECT")
    print("    SNN TRAINS FIRST (MEMORY OPTIMIZED)")
    print("="*80)
    print(f"\nConfiguration:")
    print(f"  VOC2007 Path: {voc_path}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Image Size: {CONFIG.IMGSZ}")
    print(f"  Timesteps: {CONFIG.TIMESTEPS}")
    print(f"  Gradient Accumulation: {CONFIG.GRADIENT_ACCUMULATION_STEPS}")
    print(f"  Effective Batch Size: {batch_size * CONFIG.GRADIENT_ACCUMULATION_STEPS}")
    print(f"  Epochs: {epochs}")
    print(f"  Train from Scratch: {train_from_scratch}")
    print(f"  Device: {device.upper()}")
    print(f"  Output Dir: {output_dir}")
    if device == 'cpu':
        print(f"    CPU mode - Training will be slower")
    print("="*80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Setup VOC2007
    success, voc_root = setup_voc2007_local(voc_path)
    if not success:
        print(" VOC2007 setup failed!")
        return None
    
    # STEP 1: Convert dataset
    yolo_dir = convert_voc_to_yolo_optimized(
        voc_root,
        output_dir="./yolo_voc2007",
        use_symlink=False,
        num_workers=4
    )
    
    if yolo_dir is None:
        print(" Dataset conversion failed!")
        return None
    
    # STEP 2: Create dataloaders
    train_loader, val_loader = create_dataloaders(
        yolo_dir,
        batch_size=batch_size,
        imgsz=CONFIG.IMGSZ
    )
    

    print("\n" + "="*80)
    print("STEP 3: BUILDING SNN-YOLOV8 MODEL")
    print("="*80)
    snn_model = SNNYOLOv8(nc=len(CONFIG.VOC_CLASSES), 
                          threshold=CONFIG.DEFAULT_THRESHOLD, 
                          tau=CONFIG.DEFAULT_TAU)
    total_params = sum(p.numel() for p in snn_model.parameters())
    print(f"âœ“ SNN Model built")
    print(f"  Total parameters: {total_params/1e6:.2f}M")
    print(f"  Classes: {len(CONFIG.VOC_CLASSES)}")
    print("="*80)
    
 
    print("\n" + ""*40)
    print("STEP 4: TRAINING SNN FROM SCRATCH (FIRST!)")
    print(""*40)
    
    snn_model, train_losses, firing_rates, snn_train_energy = train_snn_model(
        snn_model,
        train_loader,
        val_loader,
        epochs=epochs,
        lr=0.001,
        timesteps=CONFIG.TIMESTEPS,
        device=device
    )
    
    # Save trained SNN model
    model_path = os.path.join(output_dir, 'snn_model.pth')
    torch.save(snn_model.state_dict(), model_path)
    print(f"âœ“ SNN model saved to {model_path}")
    
    # ðŸ“Š Generate SNN training plots
    print("\n Generating SNN training plots...")
    plot_snn_training_metrics(train_losses, firing_rates, epochs, save_path=output_dir)
    plot_combined_training_comparison(train_losses, firing_rates, epochs, save_path=output_dir)
    

    snn_metrics, snn_preds, snn_targets = evaluate_snn(
        snn_model,
        val_loader,
        timesteps=CONFIG.TIMESTEPS,
        device=device,
        max_batches=eval_batches
    )
    
  
    print("\n Clearing SNN from GPU memory...")
    del snn_model
    if device == 'cuda':
        torch.cuda.empty_cache()
        print(f"âœ“ GPU memory freed")
    
    yolo_train_energy = None
    
    if train_from_scratch:
        print("\n" + ""*40)
        print("STEP 6: TRAINING YOLOV8 FROM SCRATCH")
        print(""*40)
        
        yolo_model, yolo_train_energy, yolo_results = train_yolov8_from_scratch(
            yolo_dir, 
            epochs=epochs, 
            batch_size=batch_size,
            imgsz=CONFIG.IMGSZ,
            device=device
        )
        
        # Save model
        yolo_save_path = os.path.join(output_dir, 'yolov8_trained.pt')
        yolo_model.save(yolo_save_path)
        print(f"âœ“ YOLOv8 model saved to {yolo_save_path}")
        
        # Generate YOLOv8 training plots
        print("\n Generating YOLOv8 training plots...")
        plot_yolov8_training_metrics(yolo_results, save_path=output_dir)
    else:
        # Load pretrained YOLOv8
        print("\n Loading pretrained YOLOv8...")
        from ultralytics import YOLO
        yolo_model = YOLO('yolov8n.pt')
    

    ann_metrics, ann_preds, ann_targets = evaluate_real_yolov8(
        yolo_model,
        val_loader,
        device=device,
        max_batches=eval_batches
    )
    

    comparison = compare_models(
        ann_metrics, 
        snn_metrics, 
        ann_train_energy=yolo_train_energy,
        snn_train_energy=snn_train_energy,
        save_path=output_dir
    )
    
    print("\n" + "="*80)
    print("âœ… COMPLETE PIPELINE FINISHED!")
    print("="*80)
    print(f"\nðŸ“Š Final Results:")
    
    if train_from_scratch and yolo_train_energy and snn_train_energy:
        print(f"\n  ðŸ”¥ TRAINING ENERGY:")
        print(f"    SNN:    {snn_train_energy['energy_wh']:.4f} Wh ({snn_train_energy['duration_h']:.2f} hours)")
        print(f"    YOLOv8: {yolo_train_energy['energy_wh']:.4f} Wh ({yolo_train_energy['duration_h']:.2f} hours)")
        print(f"    Savings: {comparison['train_energy_savings']:.1f}%")
    
    print(f"\n  âš¡ INFERENCE ENERGY (per image):")
    print(f"    SNN:    {snn_metrics['energy_per_image_j']:.4f} J")
    print(f"    YOLOv8: {ann_metrics['energy_per_image_j']:.4f} J")
    print(f"    Savings: {comparison['inference_energy_savings']:.1f}%")
    
    print(f"\n  ðŸŽ¯ ACCURACY:")
    print(f"    SNN mAP:    {snn_metrics['map']:.4f}")
    print(f"    YOLOv8 mAP: {ann_metrics['map']:.4f}")
    print(f"    Difference: {snn_metrics['map'] - ann_metrics['map']:+.4f}")
    
    print(f"\n  â±ï¸  INFERENCE TIME:")
    print(f"    SNN:    {snn_metrics['avg_time']*1000:.2f} ms/image")
    print(f"    YOLOv8: {ann_metrics['avg_time']*1000:.2f} ms/image")
    print(f"    Speedup: {comparison['speedup']:.2f}Ã—")
    
    print(f"\nðŸ“ Results saved to: {output_dir}/")
    print("="*80)
    
    # Reload SNN for results (optional)
    snn_model = SNNYOLOv8(nc=len(CONFIG.VOC_CLASSES))
    snn_model.load_state_dict(torch.load(model_path))
    
    results = {
        'snn_model': snn_model,
        'yolo_model': yolo_model,
        'ann_metrics': ann_metrics,
        'snn_metrics': snn_metrics,
        'yolo_train_energy': yolo_train_energy,
        'snn_train_energy': snn_train_energy,
        'train_losses': train_losses,
        'comparison': comparison,
        'train_loader': train_loader,
        'val_loader': val_loader
    }
    
    return results

# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='SNN-YOLOv8 Energy Comparison - Train Both from Scratch',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    

    )
    
    parser.add_argument('--voc_path', type=str, required=True,
                       help='Path to VOCdevkit or VOC2007 folder')
    parser.add_argument('--epochs', type=int, default=50,
                       help='Number of training epochs (default: 50)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size (default: 16)')
    parser.add_argument('--timesteps', type=int, default=4,
                       help='SNN timesteps (default: 4)')
    parser.add_argument('--eval_batches', type=int, default=50,
                       help='Number of batches for evaluation, 0=all (default: 50)')
    parser.add_argument('--train_from_scratch', action='store_true',
                       help='Train YOLOv8 from scratch (required for fair comparison)')
    parser.add_argument('--quick', action='store_true',
                       help='Quick test mode (evaluation only, pretrained YOLOv8)')
    parser.add_argument('--device', type=str, default=None, choices=['cuda', 'cpu', None],
                       help='Device to use: cuda, cpu, or None for auto-detect (default: None)')
    parser.add_argument('--output_dir', type=str, default='./results',
                       help='Directory to save results (default: ./results)')
    
    args = parser.parse_args()
    
    # Update CONFIG with command line args
    CONFIG.TIMESTEPS = args.timesteps
    
    print("\n" + "="*80)
    print("ðŸš€ SNN-YOLOV8 ENERGY COMPARISON PROJECT")
    print("="*80)
    print(f"\nConfiguration:")
    print(f"  VOC Path: {args.voc_path}")
    print(f"  Device: {args.device if args.device else 'Auto-detect'}")
    print(f"  Quick Mode: {args.quick}")
    print(f"  Train from Scratch: {args.train_from_scratch or args.quick}")
    
    if not args.quick:
        print(f"  Epochs: {args.epochs}")
        print(f"  Batch Size: {args.batch_size}")
        print(f"  Eval Batches: {args.eval_batches if args.eval_batches > 0 else 'All'}")
    print(f"  Output Dir: {args.output_dir}")
    print("="*80)
    
    # Validate configuration
    if args.train_from_scratch and args.quick:
        print("\n  Warning: --quick mode uses pretrained YOLOv8, ignoring --train_from_scratch")
        args.train_from_scratch = False
    
    # Run pipeline
    if args.quick:
        print("\n Running Quick Test Mode (Pretrained YOLOv8)...")
        results = run_complete_pipeline(
            voc_path=args.voc_path,
            batch_size=8,
            epochs=5,  # Few epochs for SNN
            eval_batches=10,
            train_from_scratch=False,
            device=args.device,
            output_dir=args.output_dir
        )
    else:
        print("\nðŸš€ Running Full Pipeline...")
        if args.train_from_scratch:
            print("âš ï¸  NOTE: Training both models from scratch will take several hours!")
            print("âš ï¸  Expected time: 4-8 hours for YOLOv8 + 2-4 hours for SNN")
        
        results = run_complete_pipeline(
            voc_path=args.voc_path,
            batch_size=args.batch_size,
            epochs=args.epochs,
            eval_batches=args.eval_batches if args.eval_batches > 0 else None,
            train_from_scratch=args.train_from_scratch,
            device=args.device,
            output_dir=args.output_dir
        )
    
    if results:
        print("\n" + "="*80)
        print(" EXECUTION COMPLETE!")
        print("="*80)
        print(f"\n Check {args.output_dir}/ folder for:")
        print(f"  âœ“ comprehensive_comparison_real_energy.png - Comparison plots")
        
        if args.train_from_scratch or not args.quick:
            print(f"  âœ“ snn_model.pth - Trained SNN model")
        
        if args.train_from_scratch:
            print(f"  âœ“ yolov8_trained.pt - YOLOv8 trained from scratch")
            print(f"  âœ“ yolov8_training/ - YOLOv8 training logs")
        
        print("\nðŸ“Š Key Findings:")
        if results['comparison']['train_energy_savings']:
            print(f"  â€¢ Training Energy Savings: {results['comparison']['train_energy_savings']:.1f}%")
        print(f"  â€¢ Inference Energy Savings: {results['comparison']['inference_energy_savings']:.1f}%")
        print(f"  â€¢ Inference Speedup: {results['comparison']['speedup']:.2f}Ã—")
        
        print("\n" + "="*80)
    else:
        print("\n Execution failed!")

        exit(1)
